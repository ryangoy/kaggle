{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents\n",
    "\n",
    "[Data Loading](#data_loading)\n",
    "\n",
    "[NLP](#nlp)\n",
    "\n",
    "[Image Processing](#cv)\n",
    "\n",
    "[Data Prep](#data_separation)\n",
    "\n",
    "[Create Model](#create_model)\n",
    "\n",
    "[Training](#training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import glob\n",
    "import os\n",
    "from os import path\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pprint\n",
    "import warnings\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "from functools import partial\n",
    "from math import sqrt\n",
    "import time\n",
    "from numba import cuda\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics import cohen_kappa_score, mean_squared_error\n",
    "from sklearn.metrics import confusion_matrix as sk_cmatrix\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold\n",
    "\n",
    "\n",
    "from keras.applications.densenet import preprocess_input, DenseNet121\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D, Input, Lambda, AveragePooling1D, \\\n",
    "        concatenate, BatchNormalization, Activation, Dropout, Embedding, Reshape\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.optimizers import *\n",
    "from keras.initializers import glorot_normal\n",
    "\n",
    "import keras.backend as K\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(seed=1)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "split_char = '/'\n",
    "\n",
    "img_size = 256\n",
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='data_loading'></a>\n",
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../input/petfinder-adoption-prediction/'\n",
    "train = pd.read_csv(path.join(data_path, 'train/train.csv'))\n",
    "test = pd.read_csv(path.join(data_path, 'test/test.csv'))\n",
    "y_trn = train['AdoptionSpeed']\n",
    "sample_submission = pd.read_csv(path.join(data_path, 'test/sample_submission.csv'))\n",
    "\n",
    "labels_breed = pd.read_csv(path.join(data_path, 'breed_labels.csv'))\n",
    "labels_state = pd.read_csv(path.join(data_path, 'color_labels.csv'))\n",
    "labels_color = pd.read_csv(path.join(data_path, 'state_labels.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filenames(mode='train'):\n",
    "    \n",
    "    image_files = sorted(glob.glob(path.join(data_path, '{}_images/*.jpg'.format(mode))))\n",
    "    metadata_files = sorted(glob.glob(path.join(data_path, '{}_metadata/*.json'.format(mode))))\n",
    "    sentiment_files = sorted(glob.glob(path.join(data_path, '{}_sentiment/*.json'.format(mode))))\n",
    "    \n",
    "    return image_files, metadata_files, sentiment_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of train images files: 58311\n",
      "num of train metadata files: 58311\n",
      "num of train sentiment files: 14442\n",
      "num of test images files: 15040\n",
      "num of test metadata files: 15040\n",
      "num of test sentiment files: 3815\n"
     ]
    }
   ],
   "source": [
    "train_image_files, train_metadata_files, train_sentiment_files = get_filenames('train')\n",
    "print('num of train images files: {}'.format(len(train_image_files)))\n",
    "print('num of train metadata files: {}'.format(len(train_metadata_files)))\n",
    "print('num of train sentiment files: {}'.format(len(train_sentiment_files)))\n",
    "\n",
    "test_image_files, test_metadata_files, test_sentiment_files = get_filenames('test')\n",
    "print('num of test images files: {}'.format(len(test_image_files)))\n",
    "print('num of test metadata files: {}'.format(len(test_metadata_files)))\n",
    "print('num of test sentiment files: {}'.format(len(test_sentiment_files)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_X = pd.read_csv('processed_data.csv')\n",
    "csv_X = csv_X.drop('Unnamed: 0', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_raw = pd.concat([train.drop('AdoptionSpeed', axis=1), test]).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in csv_X.columns:\n",
    "    if c.startswith('IMG') or c.startswith('TFIDF'):\n",
    "        csv_X = csv_X.drop(c, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='nlp'></a>\n",
    "## NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_col = 'Description'\n",
    "\n",
    "X_raw[tfidf_col] = X_raw[tfidf_col].fillna('none')\n",
    "\n",
    "# Initialize decomposition methods:\n",
    "tfv = TfidfVectorizer(min_df=1,  max_features=100,\n",
    "                      strip_accents='unicode', analyzer='word', token_pattern=r'(?u)\\b\\w+\\b',\n",
    "                      ngram_range=(1, 3))\n",
    "\n",
    "tfidf_trn = tfv.fit_transform(X_raw[tfidf_col].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df = pd.DataFrame(tfidf_trn.toarray()).add_prefix('text_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.concat([csv_X, text_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_cols = []\n",
    "for c in X.columns:\n",
    "    if c.startswith('text_'):\n",
    "        text_cols.append(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='cv'></a>\n",
    "## Image processing and densenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_to_square(im):\n",
    "    old_size = im.shape[:2]\n",
    "    ratio = float(img_size)/max(old_size)\n",
    "    new_size = tuple([int(x*ratio) for x in old_size])\n",
    "    im = cv2.resize(im, (new_size[1], new_size[0]))\n",
    "    delta_w = img_size - new_size[1]\n",
    "    delta_h = img_size - new_size[0]\n",
    "    top, bottom = delta_h//2, delta_h-(delta_h//2)\n",
    "    left, right = delta_w//2, delta_w-(delta_w//2)\n",
    "    color = [0, 0, 0]\n",
    "    new_im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT,value=color)\n",
    "    return new_im\n",
    "\n",
    "def load_image_augmented(path):\n",
    "    image = cv2.imread('{}'.format(path))\n",
    "    new_image = resize_to_square(image)\n",
    "    new_image = preprocess_input(new_image)\n",
    "    return new_image\n",
    "\n",
    "def load_image(path, pet_id):\n",
    "    image = cv2.imread('{}{}-1.jpg'.format(path, pet_id))\n",
    "    new_image = resize_to_square(image)\n",
    "    new_image = preprocess_input(new_image)\n",
    "    return new_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inp = Input((256,256,3))\n",
    "backbone = DenseNet121(input_tensor = inp, \n",
    "                       weights=\"../input/densenet/DenseNet-BC-121-32-no-top.h5\",\n",
    "                       include_top = False)\n",
    "x = backbone.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Lambda(lambda x: K.expand_dims(x,axis = -1))(x)\n",
    "x = AveragePooling1D(4)(x)\n",
    "out = Lambda(lambda x: x[:,:,0])(x)\n",
    "\n",
    "m = Model(inp,out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 228/228 [06:59<00:00,  1.84s/it]\n"
     ]
    }
   ],
   "source": [
    "n_batches = len(train_image_files) // batch_size + 1\n",
    "\n",
    "trn_img_ids = []\n",
    "trn_img_feats = []\n",
    "for b in tqdm(range(n_batches)):\n",
    "    start = b*batch_size\n",
    "    end = (b+1)*batch_size\n",
    "\n",
    "    batch_train_files = train_image_files[start:end]\n",
    "    batch_images = np.zeros((batch_size,img_size,img_size,3))\n",
    "    batch_ids = []\n",
    "    for i, img_path in enumerate(batch_train_files):\n",
    "        try:\n",
    "            batch_images[i] = load_image_augmented(img_path)\n",
    "            batch_ids.append(img_path.split('/')[-1].split('-')[0])\n",
    "        except:\n",
    "            pass\n",
    "    batch_preds = m.predict(batch_images)\n",
    "    for i,pet_id in enumerate(batch_ids):\n",
    "        trn_img_ids.append(pet_id)\n",
    "        trn_img_feats.append(batch_preds[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feats = pd.DataFrame(trn_img_feats)\n",
    "train_feats.columns = ['pic_{}'.format(str(i)) for i in range(train_feats.shape[1])]\n",
    "# train_feats['PetID'] = trn_img_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feats.insert(loc=0, column='PetID', value=trn_img_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [01:03<00:00,  1.08s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n_batches = len(test_image_files) // batch_size + 1\n",
    "\n",
    "test_img_ids = []\n",
    "test_img_feats = []\n",
    "for b in tqdm(range(n_batches)):\n",
    "    start = b*batch_size\n",
    "    end = (b+1)*batch_size\n",
    "\n",
    "    batch_test_files = test_image_files[start:end]\n",
    "    batch_images = np.zeros((batch_size,img_size,img_size,3))\n",
    "    batch_ids = []\n",
    "    for i, img_path in enumerate(batch_test_files):\n",
    "        try:\n",
    "            batch_images[i] = load_image(img_path)\n",
    "            batch_ids.append(img_path.split('/')[-1].split('-')[0])\n",
    "        except:\n",
    "            pass\n",
    "    batch_preds = m.predict(batch_images)\n",
    "    for i,pet_id in enumerate(batch_ids):\n",
    "        test_img_ids.append(pet_id)\n",
    "        test_img_feats.append(batch_preds[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pet_ids = test['PetID'].values\n",
    "# n_batches = len(pet_ids) // batch_size + 1\n",
    "\n",
    "# features = {}\n",
    "# for b in tqdm(range(n_batches)):\n",
    "#     start = b*batch_size\n",
    "#     end = (b+1)*batch_size\n",
    "#     batch_pets = pet_ids[start:end]\n",
    "#     batch_images = np.zeros((len(batch_pets),img_size,img_size,3))\n",
    "#     for i,pet_id in enumerate(batch_pets):\n",
    "#         try:\n",
    "#             batch_images[i] = load_image(\"../input/petfinder-adoption-prediction/test_images/\", pet_id)\n",
    "#         except:\n",
    "#             pass\n",
    "#     batch_preds = m.predict(batch_images)\n",
    "#     for i,pet_id in enumerate(batch_pets):\n",
    "#         features[pet_id] = batch_preds[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cuda.select_device(0)\n",
    "# cuda.close()\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_feats = pd.DataFrame(trn_img_feats)\n",
    "test_feats.columns = ['pic_{}'.format(str(i)) for i in range(test_feats.shape[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_feats.insert(loc=0, column='PetID', value=trn_img_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_feats = pd.DataFrame.from_dict(features, orient='index')\n",
    "# test_feats.columns = ['pic_{}'.format(i) for i in range(test_feats.shape[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_feats = train_feats.reset_index()\n",
    "# train_feats.rename({'index': 'PetID'}, axis='columns', inplace=True)\n",
    "\n",
    "# test_feats = test_feats.reset_index()\n",
    "# test_feats.rename({'index': 'PetID'}, axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_features = pd.concat([train_feats, test_feats], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_df = pd.DataFrame.from_dict(dict({'PetID':train.PetID, 'AdoptionSpeed': y_trn}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_trn = train_feats.merge(label_df, on='PetID', how='left')['AdoptionSpeed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_cols = []\n",
    "for c in img_features.columns:\n",
    "    if c != 'PetID':\n",
    "        img_cols.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = img_features.merge(X.reset_index(drop=True), on='PetID', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='data_separation'></a>\n",
    "## Data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = ['Type', 'Gender', 'MaturitySize', 'FurLength', 'Vaccinated', 'Dewormed', \n",
    "            'Sterilized', 'Health', 'State', 'primary_BreedName', 'secondary_BreedName',\n",
    "            'Color1', 'Color2', 'Color3']\n",
    "\n",
    "drop_cols = ['PetID']\n",
    "\n",
    "num_cols = []\n",
    "\n",
    "for c in X.columns:\n",
    "    if c not in cat_cols + img_cols + text_cols + drop_cols:\n",
    "        num_cols.append(c)\n",
    "        X[c] = (X[c]-X[c].mean())/X[c].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trn = X.iloc[:len(train_image_files)]\n",
    "X_test = X.iloc[len(train_image_files):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "rescuer_ids_augmented = X_trn.merge(train, on='PetID', how='left')['RescuerID']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='create_model'></a>\n",
    "## Create Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='training'></a>\n",
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test_model(X, y, X_test=None, num_folds=5, epochs=10, batch_size=32, use_group_kfold=True):\n",
    "    result_dict = {}\n",
    "    fold_predictions = []\n",
    "    val_preds = np.zeros((X.shape[0]))\n",
    "    \n",
    "    if use_group_kfold:\n",
    "        folds = GroupKFold(n_splits=num_folds)\n",
    "        fold_split = folds.split(X, y, rescuer_ids_augmented)\n",
    "    else:\n",
    "        folds = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=1)\n",
    "        fold_split = folds.split(X, y)\n",
    "    \n",
    "    if X_test is not None:\n",
    "        result_dict['averaged_prediction'] =  np.zeros(len(X_test))      \n",
    "        inp_test = [X_test[text_cols]] + [X_test[img_cols]] + [X_test[num_cols]] + [X_test[col] for col in cat_cols]     \n",
    "    else:\n",
    "        result_dict['averaged_prediction'] =  np.zeros(1)\n",
    "        inp_test = []\n",
    "        \n",
    "    result_dict['predictions'] = []\n",
    "    \n",
    "    for fold_n, (idx_trn, idx_val) in enumerate(fold_split):\n",
    "\n",
    "        print('Fold {}\\n'.\n",
    "              format(fold_n + 1))\n",
    "         \n",
    "        X_t = X.iloc[idx_trn]\n",
    "        X_v = X.iloc[idx_val]\n",
    "        y_t, y_v = y[idx_trn], y[idx_val]\n",
    "\n",
    "        inp_trn = [X_t[text_cols]] + [X_t[img_cols]] + [X_t[num_cols]] + [X_t[col] for col in cat_cols]\n",
    "        inp_val = [X_v[text_cols]] + [X_v[img_cols]] + [X_v[num_cols]] + [X_v[col] for col in cat_cols]\n",
    "        \n",
    "        \n",
    "        print('Beginning training...')\n",
    "        start_time = time.time()\n",
    "        \n",
    "        model = create_model(X_trn, len(text_cols), len(img_cols), len(num_cols), cat_cols, dense_dim=256, \n",
    "                 dropout=0.5, embed_factor=2, activation='relu')\n",
    "        \n",
    "        model.fit(inp_trn, y_t, batch_size=batch_size, epochs=epochs, verbose=2, validation_data=(inp_val, y_v))\n",
    "        \n",
    "        y_val_hat = model.predict(inp_val, batch_size=batch_size)\n",
    "        val_preds[idx_val] = y_val_hat[:,0]\n",
    "        \n",
    "        del X_t, X_v, inp_trn, inp_val\n",
    "        \n",
    "        if X_test is not None:\n",
    "            print('Beginning prediction...')\n",
    "            start_time = time.time()\n",
    "            y_hat = model.predict(inp_test, batch_size=batch_size)\n",
    "            result_dict['predictions'].append(y_hat)\n",
    "            result_dict['averaged_prediction'] += y_hat[:,0]\n",
    "            print('Prediction took {0:.2f} minutes.'.format((time.time()-start_time)/60))\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "    result_dict['val_preds'] = val_preds\n",
    "    if X_test is not None:\n",
    "        result_dict['averaged_prediction'] /= num_folds\n",
    "        \n",
    "    return result_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_block(x, dense_dim, dropout, activation):\n",
    "    x = Dense(dense_dim)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(activation)(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    return x\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1))\n",
    "\n",
    "def create_model(df, text_size, img_size, num_size, categorical_cols, dense_dim, \n",
    "                 dropout, embed_factor, activation):\n",
    "    \n",
    "    inputs = []\n",
    "    \n",
    "    # text \n",
    "    text_inp = Input(shape=(text_size,))\n",
    "    inputs.append(text_inp)\n",
    "    text_x = dense_block(text_inp, dense_dim, dropout, activation)\n",
    "    \n",
    "    # img\n",
    "    img_inp = Input(shape=(img_size,))\n",
    "    inputs.append(img_inp)\n",
    "    img_x = dense_block(img_inp, dense_dim, dropout, activation)\n",
    "#     img_x = dense_block(img_x, dense_dim, dropout, activation)\n",
    "\n",
    "    \n",
    "    # numerical\n",
    "    num_inp = Input(shape=(num_size,))\n",
    "    inputs.append(num_inp)\n",
    "    num_x = dense_block(num_inp, dense_dim, dropout, activation)\n",
    "    \n",
    "    # categorical\n",
    "    cat_in = []\n",
    "    cat_out = []\n",
    "    for col in categorical_cols:\n",
    "        cat_inp = Input((1,))\n",
    "        cat_in.append(cat_inp)\n",
    "        num_unique = df[col].nunique()\n",
    "        emb_size = max(num_unique//embed_factor, 2)\n",
    "        cat_emb = Embedding(num_unique, emb_size)(cat_inp)\n",
    "        cat_emb = Reshape(target_shape=(emb_size,))(cat_emb)\n",
    "        cat_out.append(cat_emb)\n",
    "    inputs += cat_in\n",
    "    cat_x = concatenate(cat_out)\n",
    "    cat_x = dense_block(cat_x, dense_dim, dropout, activation)\n",
    "\n",
    "    \n",
    "    # merge\n",
    "#     x = concatenate([text_x, img_x, cat_x])\n",
    "    x = img_x\n",
    "    x = dense_block(x, dense_dim, dropout, activation)\n",
    "    \n",
    "#     x = Dense(5, activation='sigmoid')(x)\n",
    "#     model = Model(inputs=inputs, outputs=x)  \n",
    "#     model.compile(optimizer=RMSprop(lr=0.0002), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    x = Dense(1, activation='sigmoid')(x)\n",
    "    model = Model(inputs=inputs, outputs=x)\n",
    "    model.compile(optimizer=RMSprop(lr=0.0005), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_bin = np.zeros(y_trn.shape)\n",
    "y_bin[y_trn == 4] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_0 = X_trn[y_bin==0][:13934]\n",
    "X_1 = X_trn[y_bin==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_labs = np.zeros(13934*2)\n",
    "y_labs [13934:] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "\n",
      "Beginning training...\n",
      "Train on 46648 samples, validate on 11663 samples\n",
      "Epoch 1/5\n",
      " - 3s - loss: 0.5779 - acc: 0.7401 - val_loss: 0.5437 - val_acc: 0.7411\n",
      "Epoch 2/5\n",
      " - 3s - loss: 0.5258 - acc: 0.7620 - val_loss: 0.5360 - val_acc: 0.7407\n",
      "Epoch 3/5\n",
      " - 3s - loss: 0.5099 - acc: 0.7687 - val_loss: 0.5381 - val_acc: 0.7432\n",
      "Epoch 4/5\n",
      " - 2s - loss: 0.5036 - acc: 0.7701 - val_loss: 0.5368 - val_acc: 0.7447\n",
      "Epoch 5/5\n",
      " - 2s - loss: 0.4994 - acc: 0.7708 - val_loss: 0.5610 - val_acc: 0.7432\n",
      "Beginning prediction...\n",
      "Prediction took 0.01 minutes.\n",
      "Fold 2\n",
      "\n",
      "Beginning training...\n",
      "Train on 46649 samples, validate on 11662 samples\n",
      "Epoch 1/5\n",
      " - 3s - loss: 0.5923 - acc: 0.7212 - val_loss: 0.5014 - val_acc: 0.7894\n",
      "Epoch 2/5\n",
      " - 2s - loss: 0.5356 - acc: 0.7509 - val_loss: 0.5004 - val_acc: 0.7899\n",
      "Epoch 3/5\n",
      " - 2s - loss: 0.5196 - acc: 0.7594 - val_loss: 0.5158 - val_acc: 0.7722\n",
      "Epoch 4/5\n",
      " - 2s - loss: 0.5129 - acc: 0.7606 - val_loss: 0.5108 - val_acc: 0.7703\n",
      "Epoch 5/5\n",
      " - 2s - loss: 0.5078 - acc: 0.7615 - val_loss: 0.5043 - val_acc: 0.7816\n",
      "Beginning prediction...\n",
      "Prediction took 0.01 minutes.\n",
      "Fold 3\n",
      "\n",
      "Beginning training...\n",
      "Train on 46649 samples, validate on 11662 samples\n",
      "Epoch 1/5\n",
      " - 3s - loss: 0.5894 - acc: 0.7268 - val_loss: 0.5135 - val_acc: 0.7684\n",
      "Epoch 2/5\n",
      " - 2s - loss: 0.5307 - acc: 0.7551 - val_loss: 0.5090 - val_acc: 0.7724\n",
      "Epoch 3/5\n",
      " - 2s - loss: 0.5164 - acc: 0.7627 - val_loss: 0.5060 - val_acc: 0.7729\n",
      "Epoch 4/5\n",
      " - 2s - loss: 0.5103 - acc: 0.7652 - val_loss: 0.5121 - val_acc: 0.7707\n",
      "Epoch 5/5\n",
      " - 2s - loss: 0.5059 - acc: 0.7645 - val_loss: 0.5076 - val_acc: 0.7713\n",
      "Beginning prediction...\n",
      "Prediction took 0.01 minutes.\n",
      "Fold 4\n",
      "\n",
      "Beginning training...\n",
      "Train on 46649 samples, validate on 11662 samples\n",
      "Epoch 1/5\n",
      " - 3s - loss: 0.5771 - acc: 0.7378 - val_loss: 0.5566 - val_acc: 0.7366\n",
      "Epoch 2/5\n",
      " - 2s - loss: 0.5229 - acc: 0.7644 - val_loss: 0.5484 - val_acc: 0.7395\n",
      "Epoch 3/5\n",
      " - 2s - loss: 0.5075 - acc: 0.7709 - val_loss: 0.5534 - val_acc: 0.7380\n",
      "Epoch 4/5\n",
      " - 2s - loss: 0.5011 - acc: 0.7724 - val_loss: 0.5469 - val_acc: 0.7419\n",
      "Epoch 5/5\n",
      " - 2s - loss: 0.4967 - acc: 0.7729 - val_loss: 0.5429 - val_acc: 0.7418\n",
      "Beginning prediction...\n",
      "Prediction took 0.01 minutes.\n",
      "Fold 5\n",
      "\n",
      "Beginning training...\n",
      "Train on 46649 samples, validate on 11662 samples\n",
      "Epoch 1/5\n",
      " - 3s - loss: 0.5899 - acc: 0.7270 - val_loss: 0.5117 - val_acc: 0.7668\n",
      "Epoch 2/5\n",
      " - 2s - loss: 0.5355 - acc: 0.7543 - val_loss: 0.5039 - val_acc: 0.7689\n",
      "Epoch 3/5\n",
      " - 2s - loss: 0.5189 - acc: 0.7632 - val_loss: 0.5025 - val_acc: 0.7693\n",
      "Epoch 4/5\n",
      " - 2s - loss: 0.5121 - acc: 0.7642 - val_loss: 0.5063 - val_acc: 0.7700\n",
      "Epoch 5/5\n",
      " - 2s - loss: 0.5085 - acc: 0.7681 - val_loss: 0.5051 - val_acc: 0.7681\n",
      "Beginning prediction...\n",
      "Prediction took 0.01 minutes.\n"
     ]
    }
   ],
   "source": [
    "res = train_and_test_model(X_trn, y_bin, X_test=X_test, num_folds=5, epochs=5, batch_size=64, use_group_kfold=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pred(pred):\n",
    "    sns.distplot(pred, kde=True, hist_kws={'range': [0, 5]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'res' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-8ae126de0cee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_pred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'averaged_prediction'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'res' is not defined"
     ]
    }
   ],
   "source": [
    "plot_pred(res['averaged_prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plot_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-546de454da7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_pred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_preds'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'plot_pred' is not defined"
     ]
    }
   ],
   "source": [
    "plot_pred(res['val_preds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AdoptionSpeed_nn</th>\n",
       "      <th>PetID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.181397</td>\n",
       "      <td>0008c5398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.305387</td>\n",
       "      <td>0008c5398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.302267</td>\n",
       "      <td>0008c5398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.242858</td>\n",
       "      <td>0008c5398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.267257</td>\n",
       "      <td>0008c5398</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AdoptionSpeed_nn      PetID\n",
       "0          0.181397  0008c5398\n",
       "1          0.305387  0008c5398\n",
       "2          0.302267  0008c5398\n",
       "3          0.242858  0008c5398\n",
       "4          0.267257  0008c5398"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.DataFrame({'PetID': X_test['PetID'].values, 'AdoptionSpeed_nn': res['averaged_prediction']})\n",
    "submission.to_csv('nn_test.csv', index=False)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AdoptionSpeed_nn</th>\n",
       "      <th>PetID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.226484</td>\n",
       "      <td>0008c5398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.337088</td>\n",
       "      <td>0008c5398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.355109</td>\n",
       "      <td>0008c5398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.258567</td>\n",
       "      <td>0008c5398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.304724</td>\n",
       "      <td>0008c5398</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AdoptionSpeed_nn      PetID\n",
       "0          0.226484  0008c5398\n",
       "1          0.337088  0008c5398\n",
       "2          0.355109  0008c5398\n",
       "3          0.258567  0008c5398\n",
       "4          0.304724  0008c5398"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.DataFrame({'PetID': X_trn['PetID'].values, 'AdoptionSpeed_nn': res['val_preds']})\n",
    "submission.to_csv('nn_train.csv', index=False)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
